{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy pandas transformers scikit-learn hf_xet 'accelerate>=0.26.0' datasets\n",
    "# %pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62024308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "parent_dir = os.path.abspath(os.path.join(script_dir, os.pardir))\n",
    "sys.path.append(script_dir)\n",
    "from utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import warnings\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfebf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'st1_task': True,  # Change to False for ST2\n",
    "    'enhanced_tfidf': True,\n",
    "    'feature_engineering': True,\n",
    "    'max_features': 15000,  # Increased from 10000\n",
    "    'ngram_range': (1, 3),  # Added trigrams\n",
    "    'min_df': 1,           # More inclusive\n",
    "    'max_df': 0.9,         # Less restrictive\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"\\n1. Loading data...\")\n",
    "\n",
    "DATA_PATH = \"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_PATH, \"incidents_train.csv?raw=true\"))\n",
    "valid = pd.read_csv(os.path.join(DATA_PATH, \"incidents_valid.csv?raw=true\"))\n",
    "test = pd.read_csv(os.path.join(DATA_PATH, \"incidents_test.csv?raw=true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c4ce92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCED FOOD HAZARD DETECTION - FIXED ===\n",
      "Task: ST1\n",
      "Enhanced TF-IDF: True\n",
      "Feature Engineering: True\n",
      "\n",
      "1. Loading data...\n",
      "Dataset sizes - Train: 5082, Valid: 565, Test: 997\n",
      "\n",
      "2. Enhanced text preprocessing...\n",
      "âœ… Enhanced text preprocessing completed\n",
      "\n",
      "3. Safe feature engineering...\n",
      "  Creating safe engineered features...\n",
      "  âœ… Created 28 safe engineered features\n",
      "  Feature names: ['text_length', 'word_count', 'title_length', 'title_word_count', 'title_text_ratio', 'title_words_ratio', 'year', 'month', 'day', 'is_summer']...\n",
      "\n",
      "4. Enhanced TF-IDF vectorization...\n",
      "  TF-IDF config: max_features=15000, ngrams=(1, 3)\n",
      "  âœ… TF-IDF shape: (5082, 15000)\n",
      "\n",
      "5. Combining features safely...\n",
      "  Using 28 common engineered features\n",
      "  âœ… Combined features shape: (5082, 15028)\n",
      "  TF-IDF: 15000, Engineered: 28\n",
      "\n",
      "6. Preparing labels...\n",
      "  Hazard classes: 10 (imbalance: 618.0x)\n",
      "  Product classes: 22 (imbalance: 286.8x)\n",
      "\n",
      "7. Training enhanced models...\n",
      "  Training enhanced hazard classifier...\n",
      "  Training enhanced product classifier...\n",
      "  âœ… Enhanced models trained\n",
      "\n",
      "8. Making predictions...\n",
      "\n",
      "9. Results evaluation...\n",
      "\n",
      "=== ENHANCED RESULTS ===\n",
      "\n",
      "Validation Results:\n",
      "  Hazard F1: 0.7521\n",
      "  Product F1: 0.3493\n",
      "  Final Score: 0.5507\n",
      "\n",
      "Test Results:\n",
      "  Hazard F1: 0.6146\n",
      "  Product F1: 0.4106\n",
      "  Final Score: 0.5126\n",
      "\n",
      "=== IMPROVEMENT ANALYSIS ===\n",
      "Previous baseline: 0.5978\n",
      "Enhanced model: 0.5126\n",
      "Improvement: -0.0852\n",
      "âŒ 0.0852 decrease\n",
      "\n",
      "Competition Comparison (ST1):\n",
      "  Competition BERT baseline: 0.6670\n",
      "  Competition best: 0.8223\n",
      "  Your enhanced result: 0.5126\n",
      "  Gap to BERT baseline: 0.1544\n",
      "  Gap to best result: 0.3097\n",
      "\n",
      "=== FEATURE ANALYSIS ===\n",
      "Enhanced features used:\n",
      "   1. text_length\n",
      "   2. word_count\n",
      "   3. title_length\n",
      "   4. title_word_count\n",
      "   5. title_text_ratio\n",
      "   6. title_words_ratio\n",
      "   7. year\n",
      "   8. month\n",
      "   9. day\n",
      "  10. is_summer\n",
      "  11. is_winter\n",
      "  12. is_recent\n",
      "  13. is_weekend_day\n",
      "  14. country_is_us\n",
      "  15. country_is_ca\n",
      "  ... and 13 more features\n",
      "\n",
      "Feature contribution analysis:\n",
      "  Total features: 15028\n",
      "  TF-IDF features: 15000\n",
      "  Engineered features: 28\n",
      "  Feature engineering impact: -0.0852\n",
      "\n",
      "âœ… Enhanced results saved to enhanced_safe_results_st1.json\n",
      "\n",
      "=== EXPERIMENT COMPLETED ===\n",
      "Enhanced ST1 Score: 0.5126\n",
      "Improvement: -0.0852\n",
      "\n",
      "ðŸ’¡ NEXT STEPS BASED ON RESULTS:\n",
      "âš ï¸ No improvement. Debug options:\n",
      "  1. Reduce complexity (fewer features)\n",
      "  2. Different ngram_range (1,2)\n",
      "  3. Traditional parameters\n",
      "\n",
      "ðŸŽ¯ STATUS: Need more work to reach BERT baseline.\n"
     ]
    }
   ],
   "source": [
    "task_name = \"ST1\" if CONFIG['st1_task'] else \"ST2\"\n",
    "print(\"=== ENHANCED FOOD HAZARD DETECTION - FIXED ===\")\n",
    "print(f\"Task: {task_name}\")\n",
    "print(f\"Enhanced TF-IDF: {CONFIG['enhanced_tfidf']}\")\n",
    "print(f\"Feature Engineering: {CONFIG['feature_engineering']}\")\n",
    "print(f\"Dataset sizes - Train: {len(train)}, Valid: {len(valid)}, Test: {len(test)}\")\n",
    "\n",
    "# Task selection\n",
    "if CONFIG['st1_task']:\n",
    "    hazard_col = 'hazard-category'\n",
    "    product_col = 'product-category'\n",
    "else:\n",
    "    hazard_col = 'hazard'\n",
    "    product_col = 'product'\n",
    "\n",
    "# 2. Enhanced Text Preprocessing\n",
    "print(\"\\n2. Enhanced text preprocessing...\")\n",
    "\n",
    "# Apply enhanced preprocessing\n",
    "train = enhanced_text_preparation(train)\n",
    "valid = enhanced_text_preparation(valid)\n",
    "test = enhanced_text_preparation(test)\n",
    "\n",
    "print(\"Enhanced text preprocessing completed\")\n",
    "\n",
    "# 3. Safe Feature Engineering (no problematic features)\n",
    "print(\"\\n3. Safe feature engineering...\")\n",
    "\n",
    "if CONFIG['feature_engineering']:\n",
    "    print(\"  Creating safe engineered features...\")\n",
    "    train_features = create_safe_features(train)\n",
    "    valid_features = create_safe_features(valid)\n",
    "    test_features = create_safe_features(test)\n",
    "    \n",
    "    print(f\"Created {train_features.shape[1]} safe engineered features\")\n",
    "    print(f\"Feature names: {list(train_features.columns[:10])}...\")\n",
    "\n",
    "# 4. Enhanced TF-IDF\n",
    "print(\"\\n4. Enhanced TF-IDF vectorization...\")\n",
    "\n",
    "if CONFIG['enhanced_tfidf']:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=CONFIG['max_features'],\n",
    "        ngram_range=CONFIG['ngram_range'],\n",
    "        min_df=CONFIG['min_df'],\n",
    "        max_df=CONFIG['max_df'],\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        norm='l2',\n",
    "        smooth_idf=True\n",
    "    )\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        stop_words='english'\n",
    "    )\n",
    "\n",
    "print(f\"  TF-IDF config: max_features={CONFIG['max_features']}, ngrams={CONFIG['ngram_range']}\")\n",
    "\n",
    "# Create TF-IDF features\n",
    "X_train_tfidf = vectorizer.fit_transform(train['combined_text'])\n",
    "X_valid_tfidf = vectorizer.transform(valid['combined_text'])\n",
    "X_test_tfidf = vectorizer.transform(test['combined_text'])\n",
    "\n",
    "print(f\"TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# 5. Safe Feature Combination\n",
    "print(\"\\n5. Combining features safely...\")\n",
    "\n",
    "if CONFIG['feature_engineering']:\n",
    "    # Ensure all feature sets have the same columns\n",
    "    common_columns = train_features.columns.intersection(valid_features.columns).intersection(test_features.columns)\n",
    "    \n",
    "    train_features_safe = train_features[common_columns]\n",
    "    valid_features_safe = valid_features[common_columns]\n",
    "    test_features_safe = test_features[common_columns]\n",
    "    \n",
    "    print(f\"  Using {len(common_columns)} common engineered features\")\n",
    "    \n",
    "    # Scale engineered features\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features_safe)\n",
    "    valid_features_scaled = scaler.transform(valid_features_safe)\n",
    "    test_features_scaled = scaler.transform(test_features_safe)\n",
    "    \n",
    "    # Combine TF-IDF + engineered features\n",
    "    X_train = hstack([X_train_tfidf, train_features_scaled])\n",
    "    X_valid = hstack([X_valid_tfidf, valid_features_scaled])\n",
    "    X_test = hstack([X_test_tfidf, test_features_scaled])\n",
    "    \n",
    "    print(f\"Combined features shape: {X_train.shape}\")\n",
    "    print(f\"TF-IDF: {X_train_tfidf.shape[1]}, Engineered: {len(common_columns)}\")\n",
    "else:\n",
    "    X_train = X_train_tfidf\n",
    "    X_valid = X_valid_tfidf\n",
    "    X_test = X_test_tfidf\n",
    "\n",
    "# 6. Prepare Labels\n",
    "print(\"\\n6. Preparing labels...\")\n",
    "\n",
    "y_train_hazard = train[hazard_col].values\n",
    "y_valid_hazard = valid[hazard_col].values\n",
    "y_test_hazard = test[hazard_col].values\n",
    "\n",
    "y_train_product = train[product_col].values\n",
    "y_valid_product = valid[product_col].values\n",
    "y_test_product = test[product_col].values\n",
    "\n",
    "# Show class distribution\n",
    "hazard_counts = pd.Series(y_train_hazard).value_counts()\n",
    "product_counts = pd.Series(y_train_product).value_counts()\n",
    "\n",
    "print(f\"  Hazard classes: {len(hazard_counts)} (imbalance: {hazard_counts.iloc[0]/hazard_counts.iloc[-1]:.1f}x)\")\n",
    "print(f\"  Product classes: {len(product_counts)} (imbalance: {product_counts.iloc[0]/product_counts.iloc[-1]:.1f}x)\")\n",
    "\n",
    "# 7. Enhanced Model Training\n",
    "print(\"\\n7. Training enhanced models...\")\n",
    "\n",
    "# Class weights\n",
    "hazard_classes = np.unique(y_train_hazard)\n",
    "product_classes = np.unique(y_train_product)\n",
    "\n",
    "hazard_weights = compute_class_weight('balanced', classes=hazard_classes, y=y_train_hazard)\n",
    "product_weights = compute_class_weight('balanced', classes=product_classes, y=y_train_product)\n",
    "\n",
    "hazard_weight_dict = dict(zip(hazard_classes, hazard_weights))\n",
    "product_weight_dict = dict(zip(product_classes, product_weights))\n",
    "\n",
    "# Enhanced Logistic Regression models\n",
    "print(\"  Training enhanced hazard classifier...\")\n",
    "hazard_model = LogisticRegression(\n",
    "    class_weight=hazard_weight_dict,\n",
    "    max_iter=2000,\n",
    "    C=1.0,\n",
    "    solver='liblinear',\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "hazard_model.fit(X_train, y_train_hazard)\n",
    "\n",
    "print(\"  Training enhanced product classifier...\")\n",
    "product_model = LogisticRegression(\n",
    "    class_weight=product_weight_dict,\n",
    "    max_iter=2000,\n",
    "    C=1.0,\n",
    "    solver='liblinear',\n",
    "    random_state=CONFIG['random_state']\n",
    ")\n",
    "product_model.fit(X_train, y_train_product)\n",
    "\n",
    "print(\"Enhanced models trained\")\n",
    "\n",
    "# 8. Predictions\n",
    "print(\"\\n8. Making predictions...\")\n",
    "\n",
    "hazard_pred_valid = hazard_model.predict(X_valid)\n",
    "product_pred_valid = product_model.predict(X_valid)\n",
    "\n",
    "hazard_pred_test = hazard_model.predict(X_test)\n",
    "product_pred_test = product_model.predict(X_test)\n",
    "\n",
    "print(\"\\n9. Results evaluation...\")\n",
    "\n",
    "# Validation results\n",
    "valid_scores = compute_food_hazard_score(\n",
    "    y_valid_hazard, y_valid_product,\n",
    "    hazard_pred_valid, product_pred_valid\n",
    ")\n",
    "\n",
    "# Test results\n",
    "test_scores = compute_food_hazard_score(\n",
    "    y_test_hazard, y_test_product,\n",
    "    hazard_pred_test, product_pred_test\n",
    ")\n",
    "\n",
    "print(\"\\n=== ENHANCED RESULTS ===\")\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"Hazard F1: {valid_scores['f1_hazards']:.4f}\")\n",
    "print(f\"Product F1: {valid_scores['f1_products']:.4f}\")\n",
    "print(f\"Final Score: {valid_scores['final_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Hazard F1: {test_scores['f1_hazards']:.4f}\")\n",
    "print(f\"Product F1: {test_scores['f1_products']:.4f}\")\n",
    "print(f\"Final Score: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "# 10. Comparison with Previous Results\n",
    "print(f\"\\n=== IMPROVEMENT ANALYSIS ===\")\n",
    "\n",
    "# Previous baseline results (from your runs)\n",
    "if CONFIG['st1_task']:\n",
    "    previous_score = 0.5978\n",
    "    competition_bert = 0.667\n",
    "    competition_best = 0.8223\n",
    "else:\n",
    "    previous_score = 0.2546\n",
    "    competition_bert = 0.498\n",
    "    competition_best = 0.5473\n",
    "\n",
    "improvement = test_scores['final_score'] - previous_score\n",
    "print(f\"Previous baseline: {previous_score:.4f}\")\n",
    "print(f\"Enhanced model: {test_scores['final_score']:.4f}\")\n",
    "print(f\"Improvement: {improvement:+.4f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"{improvement:.4f} improvement achieved!\")\n",
    "    if improvement > 0.05:\n",
    "        print(f\"SIGNIFICANT improvement!\")\n",
    "else:\n",
    "    print(f\"{abs(improvement):.4f} decrease\")\n",
    "\n",
    "print(f\"\\nCompetition Comparison ({task_name}):\")\n",
    "print(f\"Competition BERT baseline: {competition_bert:.4f}\")\n",
    "print(f\"Competition best: {competition_best:.4f}\")\n",
    "print(f\"Your enhanced result: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "if test_scores['final_score'] > competition_bert:\n",
    "    print(f\"You beat the BERT baseline by {test_scores['final_score'] - competition_bert:.4f}!\")\n",
    "else:\n",
    "    gap = competition_bert - test_scores['final_score']\n",
    "    print(f\"Gap to BERT baseline: {gap:.4f}\")\n",
    "\n",
    "gap_to_best = competition_best - test_scores['final_score']\n",
    "print(f\"Gap to best result: {gap_to_best:.4f}\")\n",
    "\n",
    "# 11. Feature Analysis\n",
    "print(f\"\\n=== FEATURE ANALYSIS ===\")\n",
    "\n",
    "if CONFIG['feature_engineering']:\n",
    "    print(f\"Enhanced features used:\")\n",
    "    feature_list = train_features_safe.columns.tolist()\n",
    "    for i, feat in enumerate(feature_list):\n",
    "        if i < 15:  # Show first 15\n",
    "            print(f\"  {i+1:2d}. {feat}\")\n",
    "        elif i == 15:\n",
    "            print(f\"... and {len(feature_list)-15} more features\")\n",
    "            break\n",
    "\n",
    "# Quick feature effectiveness test\n",
    "print(f\"\\nFeature contribution analysis:\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "if CONFIG['feature_engineering']:\n",
    "    print(f\"Engineered features: {len(common_columns)}\")\n",
    "    print(f\"Feature engineering impact: {improvement:.4f}\")\n",
    "\n",
    "# 12. Save Results\n",
    "results_summary = {\n",
    "    'task': task_name,\n",
    "    'method': 'Enhanced TF-IDF + LogReg + Safe Feature Engineering',\n",
    "    'config': CONFIG,\n",
    "    'improvements': {\n",
    "        'enhanced_tfidf': CONFIG['enhanced_tfidf'],\n",
    "        'feature_engineering': CONFIG['feature_engineering'],\n",
    "        'total_features': X_train.shape[1],\n",
    "        'tfidf_features': X_train_tfidf.shape[1],\n",
    "        'engineered_features': len(common_columns) if CONFIG['feature_engineering'] else 0\n",
    "    },\n",
    "    'results': {\n",
    "        'validation': valid_scores,\n",
    "        'test': test_scores,\n",
    "        'improvement_over_baseline': float(improvement),\n",
    "        'previous_baseline': float(previous_score)\n",
    "    },\n",
    "    'competition_comparison': {\n",
    "        'competition_bert': float(competition_bert),\n",
    "        'competition_best': float(competition_best),\n",
    "        'beats_bert_baseline': bool(test_scores['final_score'] > competition_bert),\n",
    "        'gap_to_best': float(gap_to_best)\n",
    "    }\n",
    "}\n",
    "\n",
    "filename = f\"enhanced_safe_results_{task_name.lower()}.json\"\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nEnhanced results saved to {filename}\")\n",
    "\n",
    "print(\"\\n=== EXPERIMENT COMPLETED ===\")\n",
    "print(f\"Enhanced {task_name} Score: {test_scores['final_score']:.4f}\")\n",
    "print(f\"Improvement: {improvement:+.4f}\")\n",
    "\n",
    "# Actionable next steps\n",
    "print(f\"\\nNEXT STEPS BASED ON RESULTS:\")\n",
    "if improvement > 0.05:\n",
    "    print(\"EXCELLENT improvement! Ready for:\")\n",
    "    print(\"1. XGBoost ensemble (combine with LogReg)\")\n",
    "    print(\"2. Try ST2 with same approach\")\n",
    "    print(\"3. Advanced feature engineering\")\n",
    "elif improvement > 0:\n",
    "    print(\"Good improvement! Try:\")\n",
    "    print(\"1. XGBoost model\")\n",
    "    print(\"2. Different TF-IDF parameters\")\n",
    "    print(\"3. Ensemble methods\")\n",
    "else:\n",
    "    print(\"No improvement. Debug options:\")\n",
    "    print(\"1. Reduce complexity (fewer features)\")\n",
    "    print(\"2. Different ngram_range (1,2)\")\n",
    "    print(\"3. Traditional parameters\")\n",
    "\n",
    "# Performance summary\n",
    "if CONFIG['st1_task']:\n",
    "    if test_scores['final_score'] > 0.67:\n",
    "        print(\"\\nSTATUS: BERT baseline beaten! Ready for advanced techniques.\")\n",
    "    elif test_scores['final_score'] > 0.63:\n",
    "        print(\"\\nSTATUS: Close to BERT baseline. One more improvement should do it.\")\n",
    "    else:\n",
    "        print(\"\\nSTATUS: Need more work to reach BERT baseline.\")\n",
    "else:\n",
    "    if test_scores['final_score'] > 0.35:\n",
    "        print(\"\\nSTATUS: Good progress on difficult ST2 task.\")\n",
    "    else:\n",
    "        print(\"\\nSTATUS: ST2 still challenging, try data augmentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back-to-basics + XGBoost approach\n",
    "CONFIG = {\n",
    "    'st1_task': True,  # Change to False for ST2\n",
    "    'models_to_try': ['logreg', 'xgb'],  # Which models to test\n",
    "    'ensemble': True,   # Whether to combine models\n",
    "    'grid_search_tfidf': True,  # Try different TF-IDF params\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa8b24c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "task_name = \"ST1\" if CONFIG['st1_task'] else \"ST2\"\n",
    "print(\"=== OPTIMIZED SIMPLE MODEL + ENSEMBLE ===\")\n",
    "print(f\"Task: {task_name}\")\n",
    "print(f\"Models to try: {CONFIG['models_to_try']}\")\n",
    "print(f\"Ensemble: {CONFIG['ensemble']}\")\n",
    "\n",
    "# Task selection\n",
    "if CONFIG['st1_task']:\n",
    "    hazard_col = 'hazard-category'\n",
    "    product_col = 'product-category'\n",
    "else:\n",
    "    hazard_col = 'hazard'\n",
    "    product_col = 'product'\n",
    "\n",
    "print(f\"Dataset sizes - Train: {len(train)}, Valid: {len(valid)}, Test: {len(test)}\")\n",
    "\n",
    "# 2. Simple but effective text preprocessing\n",
    "print(\"\\n2. Simple text preprocessing...\")\n",
    "\n",
    "train = simple_but_effective_preprocessing(train)\n",
    "valid = simple_but_effective_preprocessing(valid)\n",
    "test = simple_but_effective_preprocessing(test)\n",
    "\n",
    "print(\"Simple preprocessing completed\")\n",
    "\n",
    "# 3. TF-IDF Parameter Grid Search\n",
    "print(\"\\n3. TF-IDF parameter optimization...\")\n",
    "\n",
    "if CONFIG['grid_search_tfidf']:\n",
    "    # Test different TF-IDF configurations\n",
    "    tfidf_configs = [\n",
    "        {'name': 'original', 'max_features': 10000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95},\n",
    "        {'name': 'more_features', 'max_features': 15000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95},\n",
    "        {'name': 'trigrams', 'max_features': 10000, 'ngram_range': (1, 3), 'min_df': 2, 'max_df': 0.95},\n",
    "        {'name': 'less_restrictive', 'max_features': 10000, 'ngram_range': (1, 2), 'min_df': 1, 'max_df': 0.9},\n",
    "        {'name': 'balanced', 'max_features': 12000, 'ngram_range': (1, 2), 'min_df': 1, 'max_df': 0.92}\n",
    "    ]\n",
    "    \n",
    "    best_config = None\n",
    "    best_score = 0\n",
    "    tfidf_results = []\n",
    "    \n",
    "    # Labels for quick validation\n",
    "    y_train_hazard = train[hazard_col].values\n",
    "    y_valid_hazard = valid[hazard_col].values\n",
    "    y_train_product = train[product_col].values\n",
    "    y_valid_product = valid[product_col].values\n",
    "    \n",
    "    print(\"Testing TF-IDF configurations...\")\n",
    "    \n",
    "    for config in tfidf_configs:\n",
    "        print(f\"Testing {config['name']}...\")\n",
    "        \n",
    "        # Create vectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=config['max_features'],\n",
    "            ngram_range=config['ngram_range'],\n",
    "            min_df=config['min_df'],\n",
    "            max_df=config['max_df'],\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        # Fit and transform\n",
    "        X_train_temp = vectorizer.fit_transform(train['combined_text'])\n",
    "        X_valid_temp = vectorizer.transform(valid['combined_text'])\n",
    "        \n",
    "        # Quick LogReg test\n",
    "        hazard_classes = np.unique(y_train_hazard)\n",
    "        hazard_weights = compute_class_weight('balanced', classes=hazard_classes, y=y_train_hazard)\n",
    "        hazard_weight_dict = dict(zip(hazard_classes, hazard_weights))\n",
    "        \n",
    "        quick_model = LogisticRegression(\n",
    "            class_weight=hazard_weight_dict,\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        quick_model.fit(X_train_temp, y_train_hazard)\n",
    "        \n",
    "        # Quick evaluation\n",
    "        hazard_pred_temp = quick_model.predict(X_valid_temp)\n",
    "        quick_f1 = f1_score(y_valid_hazard, hazard_pred_temp, average='macro')\n",
    "        \n",
    "        tfidf_results.append({\n",
    "            'config': config['name'],\n",
    "            'f1_hazard': quick_f1,\n",
    "            'params': config\n",
    "        })\n",
    "        \n",
    "        print(f\"      {config['name']}: Hazard F1 = {quick_f1:.4f}\")\n",
    "        \n",
    "        if quick_f1 > best_score:\n",
    "            best_score = quick_f1\n",
    "            best_config = config\n",
    "    \n",
    "    print(f\"Best TF-IDF config: {best_config['name']} (F1: {best_score:.4f})\")\n",
    "    \n",
    "else:\n",
    "    # Use original configuration\n",
    "    best_config = {'max_features': 10000, 'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.95}\n",
    "\n",
    "# 4. Create final TF-IDF features\n",
    "print(\"\\n4. Creating final TF-IDF features...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=best_config['max_features'],\n",
    "    ngram_range=best_config['ngram_range'],\n",
    "    min_df=best_config['min_df'],\n",
    "    max_df=best_config['max_df'],\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train['combined_text'])\n",
    "X_valid = vectorizer.transform(valid['combined_text'])\n",
    "X_test = vectorizer.transform(test['combined_text'])\n",
    "\n",
    "print(f\"Final TF-IDF shape: {X_train.shape}\")\n",
    "\n",
    "# 5. Prepare labels\n",
    "y_train_hazard = train[hazard_col].values\n",
    "y_valid_hazard = valid[hazard_col].values\n",
    "y_test_hazard = test[hazard_col].values\n",
    "\n",
    "y_train_product = train[product_col].values\n",
    "y_valid_product = valid[product_col].values\n",
    "y_test_product = test[product_col].values\n",
    "\n",
    "# 6. Train Multiple Models\n",
    "print(\"\\n5. Training multiple models...\")\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# Class weights\n",
    "hazard_classes = np.unique(y_train_hazard)\n",
    "product_classes = np.unique(y_train_product)\n",
    "\n",
    "hazard_weights = compute_class_weight('balanced', classes=hazard_classes, y=y_train_hazard)\n",
    "product_weights = compute_class_weight('balanced', classes=product_classes, y=y_train_product)\n",
    "\n",
    "hazard_weight_dict = dict(zip(hazard_classes, hazard_weights))\n",
    "product_weight_dict = dict(zip(product_classes, product_weights))\n",
    "\n",
    "# Logistic Regression\n",
    "if 'logreg' in CONFIG['models_to_try']:\n",
    "    print(\"  Training Logistic Regression...\")\n",
    "    \n",
    "    models['logreg_hazard'] = LogisticRegression(\n",
    "        class_weight=hazard_weight_dict,\n",
    "        max_iter=1000,\n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    models['logreg_product'] = LogisticRegression(\n",
    "        class_weight=product_weight_dict,\n",
    "        max_iter=1000,\n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    models['logreg_hazard'].fit(X_train, y_train_hazard)\n",
    "    models['logreg_product'].fit(X_train, y_train_product)\n",
    "    \n",
    "    predictions['logreg'] = {\n",
    "        'hazard_valid': models['logreg_hazard'].predict(X_valid),\n",
    "        'product_valid': models['logreg_product'].predict(X_valid),\n",
    "        'hazard_test': models['logreg_hazard'].predict(X_test),\n",
    "        'product_test': models['logreg_product'].predict(X_test)\n",
    "    }\n",
    "\n",
    "# XGBoost\n",
    "if 'xgb' in CONFIG['models_to_try']:\n",
    "    print(\"  Training XGBoost...\")\n",
    "    \n",
    "    # XGBoost with class weights\n",
    "    models['xgb_hazard'] = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    models['xgb_product'] = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Convert sparse matrix to dense for XGBoost\n",
    "    X_train_dense = X_train.toarray()\n",
    "    X_valid_dense = X_valid.toarray()\n",
    "    X_test_dense = X_test.toarray()\n",
    "    \n",
    "    # Calculate sample weights for XGBoost\n",
    "    hazard_sample_weights = np.array([hazard_weight_dict[y] for y in y_train_hazard])\n",
    "    product_sample_weights = np.array([product_weight_dict[y] for y in y_train_product])\n",
    "    \n",
    "    models['xgb_hazard'].fit(X_train_dense, y_train_hazard, sample_weight=hazard_sample_weights)\n",
    "    models['xgb_product'].fit(X_train_dense, y_train_product, sample_weight=product_sample_weights)\n",
    "    \n",
    "    predictions['xgb'] = {\n",
    "        'hazard_valid': models['xgb_hazard'].predict(X_valid_dense),\n",
    "        'product_valid': models['xgb_product'].predict(X_valid_dense),\n",
    "        'hazard_test': models['xgb_hazard'].predict(X_test_dense),\n",
    "        'product_test': models['xgb_product'].predict(X_test_dense)\n",
    "    }\n",
    "\n",
    "print(\"All models trained\")\n",
    "\n",
    "# 7. Individual Model Evaluation\n",
    "print(\"\\n6. Individual model evaluation...\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name in predictions.keys():\n",
    "    print(f\"\\n  {model_name.upper()} Results:\")\n",
    "    \n",
    "    # Validation\n",
    "    valid_scores = compute_food_hazard_score(\n",
    "        y_valid_hazard, y_valid_product,\n",
    "        predictions[model_name]['hazard_valid'],\n",
    "        predictions[model_name]['product_valid']\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    test_scores = compute_food_hazard_score(\n",
    "        y_test_hazard, y_test_product,\n",
    "        predictions[model_name]['hazard_test'],\n",
    "        predictions[model_name]['product_test']\n",
    "    )\n",
    "    \n",
    "    model_results[model_name] = {\n",
    "        'validation': valid_scores,\n",
    "        'test': test_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"Validation: {valid_scores['final_score']:.4f} (H: {valid_scores['f1_hazards']:.4f}, P: {valid_scores['f1_products']:.4f})\")\n",
    "    print(f\"Test: {test_scores['final_score']:.4f} (H: {test_scores['f1_hazards']:.4f}, P: {test_scores['f1_products']:.4f})\")\n",
    "\n",
    "# 8. Ensemble if requested\n",
    "if CONFIG['ensemble'] and len(predictions) > 1:\n",
    "    print(\"\\n7. Ensemble combination...\")\n",
    "    \n",
    "    from scipy import stats\n",
    "    \n",
    "    # Simple majority voting\n",
    "    ensemble_hazard_valid = []\n",
    "    ensemble_product_valid = []\n",
    "    ensemble_hazard_test = []\n",
    "    ensemble_product_test = []\n",
    "    \n",
    "    for i in range(len(y_valid_hazard)):\n",
    "        hazard_votes = [predictions[model]['hazard_valid'][i] for model in predictions.keys()]\n",
    "        ensemble_hazard_valid.append(stats.mode(hazard_votes, keepdims=True).mode[0])\n",
    "        \n",
    "        product_votes = [predictions[model]['product_valid'][i] for model in predictions.keys()]\n",
    "        ensemble_product_valid.append(stats.mode(product_votes, keepdims=True).mode[0])\n",
    "    \n",
    "    for i in range(len(y_test_hazard)):\n",
    "        hazard_votes = [predictions[model]['hazard_test'][i] for model in predictions.keys()]\n",
    "        ensemble_hazard_test.append(stats.mode(hazard_votes, keepdims=True).mode[0])\n",
    "        \n",
    "        product_votes = [predictions[model]['product_test'][i] for model in predictions.keys()]\n",
    "        ensemble_product_test.append(stats.mode(product_votes, keepdims=True).mode[0])\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    ensemble_valid = compute_food_hazard_score(\n",
    "        y_valid_hazard, y_valid_product,\n",
    "        np.array(ensemble_hazard_valid), np.array(ensemble_product_valid)\n",
    "    )\n",
    "    \n",
    "    ensemble_test = compute_food_hazard_score(\n",
    "        y_test_hazard, y_test_product,\n",
    "        np.array(ensemble_hazard_test), np.array(ensemble_product_test)\n",
    "    )\n",
    "    \n",
    "    model_results['ensemble'] = {\n",
    "        'validation': ensemble_valid,\n",
    "        'test': ensemble_test\n",
    "    }\n",
    "    \n",
    "    print(f\"ENSEMBLE Results:\")\n",
    "    print(f\"Validation: {ensemble_valid['final_score']:.4f} (H: {ensemble_valid['f1_hazards']:.4f}, P: {ensemble_valid['f1_products']:.4f})\")\n",
    "    print(f\"Test: {ensemble_test['final_score']:.4f} (H: {ensemble_test['f1_hazards']:.4f}, P: {ensemble_test['f1_products']:.4f})\")\n",
    "\n",
    "# 9. Best Model Selection and Final Analysis\n",
    "print(f\"\\n=== FINAL RESULTS COMPARISON ===\")\n",
    "\n",
    "# Find best model\n",
    "best_model = max(model_results.keys(), key=lambda x: model_results[x]['test']['final_score'])\n",
    "best_score = model_results[best_model]['test']['final_score']\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "for model_name, results in model_results.items():\n",
    "    test_score = results['test']['final_score']\n",
    "    print(f\"  {model_name:15s}: {test_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model.upper()}\")\n",
    "print(f\"Best Test Score: {best_score:.4f}\")\n",
    "\n",
    "# Comparison with previous results\n",
    "previous_baseline = 0.5978  # Original TF-IDF result\n",
    "improvement = best_score - previous_baseline\n",
    "\n",
    "print(f\"\\n=== IMPROVEMENT ANALYSIS ===\")\n",
    "print(f\"Previous baseline: {previous_baseline:.4f}\")\n",
    "print(f\"Best new model: {best_score:.4f}\")\n",
    "print(f\"Improvement: {improvement:+.4f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"{improvement:.4f} improvement achieved!\")\n",
    "    if improvement > 0.05:\n",
    "        print(f\"SIGNIFICANT improvement!\")\n",
    "else:\n",
    "    print(f\"{abs(improvement):.4f} decrease\")\n",
    "\n",
    "# Competition comparison\n",
    "if CONFIG['st1_task']:\n",
    "    competition_bert = 0.667\n",
    "    competition_best = 0.8223\n",
    "else:\n",
    "    competition_bert = 0.498\n",
    "    competition_best = 0.5473\n",
    "\n",
    "print(f\"\\nCompetition Comparison ({task_name}):\")\n",
    "print(f\"Competition BERT baseline: {competition_bert:.4f}\")\n",
    "print(f\"Competition best: {competition_best:.4f}\")\n",
    "print(f\"Your best result: {best_score:.4f}\")\n",
    "\n",
    "if best_score > competition_bert:\n",
    "    print(f\"You beat the BERT baseline by {best_score - competition_bert:.4f}!\")\n",
    "else:\n",
    "    gap = competition_bert - best_score\n",
    "    print(f\"Gap to BERT baseline: {gap:.4f}\")\n",
    "\n",
    "# 10. Save Results\n",
    "results_summary = {\n",
    "    'task': task_name,\n",
    "    'method': 'Optimized Simple + Ensemble',\n",
    "    'config': CONFIG,\n",
    "    'tfidf_optimization': tfidf_results if CONFIG['grid_search_tfidf'] else None,\n",
    "    'best_tfidf_config': best_config,\n",
    "    'model_results': model_results,\n",
    "    'best_model': best_model,\n",
    "    'best_score': float(best_score),\n",
    "    'improvement_over_baseline': float(improvement),\n",
    "    'competition_comparison': {\n",
    "        'beats_bert_baseline': bool(best_score > competition_bert),\n",
    "        'gap_to_best': float(competition_best - best_score)\n",
    "    }\n",
    "}\n",
    "\n",
    "filename = f'optimized_results_{task_name.lower()}.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to {filename}\")\n",
    "\n",
    "print(f\"\\n=== EXPERIMENT COMPLETED ===\")\n",
    "print(f\"Best {task_name} Score: {best_score:.4f}\")\n",
    "print(f\"Best Model: {best_model}\")\n",
    "\n",
    "# Next steps recommendation\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "if best_score > competition_bert:\n",
    "    print(\"EXCELLENT! You beat BERT baseline. Try:\")\n",
    "    print(\"1. ST2 with same approach\")\n",
    "    print(\"2. Data augmentation for even better results\")\n",
    "    print(\"3. More advanced ensemble methods\")\n",
    "elif improvement > 0.03:\n",
    "    print(\"Good progress! Try:\")\n",
    "    print(\"1. More XGBoost hyperparameter tuning\")\n",
    "    print(\"2. Different ensemble methods (weighted voting)\")\n",
    "    print(\"3. ST2 application\")\n",
    "else:\n",
    "    print(\"Need different approach. Consider:\")\n",
    "    print(\"1. Data augmentation\")\n",
    "    print(\"2. Different text preprocessing\")\n",
    "    print(\"3. Alternative models (Random Forest)\")\n",
    "\n",
    "print(f\"\\nCurrent distance to competition:\")\n",
    "print(f\"BERT baseline: {max(0, competition_bert - best_score):.3f} points away\")\n",
    "print(f\"Best result: {competition_best - best_score:.3f} points away\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
