{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f58c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/michal/miniconda/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in /Users/michal/miniconda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/michal/miniconda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: hf_xet in /Users/michal/miniconda/lib/python3.12/site-packages (1.0.3)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/michal/miniconda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in /Users/michal/miniconda/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michal/miniconda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in /Users/michal/miniconda/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /Users/michal/miniconda/lib/python3.12/site-packages (4.51.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed transformers-4.52.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas transformers scikit-learn hf_xet 'accelerate>=0.26.0' datasets\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f75a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/1d/k2nxt21n3qn7tn1clzr__dkm0000gq/T/ipykernel_56752/3740747211.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/michal/miniconda/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/michal/miniconda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE FOOD HAZARD DETECTION - COMPATIBILITY MODE ===\n",
      "PyTorch version: 2.2.2\n",
      "Task: ST1\n",
      "Method: TF-IDF + LogReg\n",
      "\n",
      "1. Loading data...\n",
      "‚úÖ Data loaded - Train: 5082, Valid: 565, Test: 997\n",
      "Target columns: hazard-category, product-category\n",
      "\n",
      "2. Data analysis...\n",
      "Hazard classes: 10\n",
      "Product classes: 22\n",
      "Most common hazard: allergens (1854 samples)\n",
      "Most common product: meat, egg and dairy products (1434 samples)\n",
      "Hazard imbalance ratio: 618.0x\n",
      "Product imbalance ratio: 286.8x\n",
      "\n",
      "3. Text preparation...\n",
      "‚úÖ Texts prepared - Average length: 169.4 words\n",
      "\n",
      "5. Training TF-IDF + LogReg model...\n",
      "  Creating TF-IDF features...\n",
      "  ‚úÖ TF-IDF shape: (5082, 10000)\n",
      "  Class weights computed - Hazard: 10, Product: 22\n",
      "  Training hazard classifier...\n",
      "  Training product classifier...\n",
      "  ‚úÖ Models trained\n",
      "\n",
      "6. Making predictions...\n",
      "\n",
      "7. Evaluation...\n",
      "\n",
      "=== VALIDATION RESULTS ===\n",
      "Hazard F1: 0.6892\n",
      "Product F1: 0.5335\n",
      "Final Score: 0.6113\n",
      "\n",
      "=== TEST RESULTS ===\n",
      "Hazard F1: 0.6173\n",
      "Product F1: 0.5783\n",
      "Final Score: 0.5978\n",
      "\n",
      "8. Baseline comparison...\n",
      "Majority Baseline: 0.0352\n",
      "Our Model: 0.5978\n",
      "Improvement: +0.5626\n",
      "\n",
      "=== COMPETITION COMPARISON (ST1) ===\n",
      "Competition Best (Anastasia): 0.8223\n",
      "Competition BERT Baseline: 0.6670\n",
      "Your Result: 0.5978\n",
      "Gap to best: 0.2245\n",
      "\n",
      "‚úÖ Results saved to simple_results_st1.json\n",
      "\n",
      "=== EXPERIMENT COMPLETED ===\n",
      "Method: TF-IDF + Logistic Regression\n",
      "Final ST1 Score: 0.5978\n",
      "\n",
      "üí° NEXT STEPS FOR BETTER RESULTS:\n",
      "1. Try different TF-IDF parameters (max_features, ngram_range)\n",
      "2. Add more feature engineering (text length, country, date)\n",
      "3. Try ensemble methods (combine multiple models)\n",
      "4. If BERT works, try: 'test_bert_loading': True\n",
      "5. For ST2 task, set: 'st1_task': False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== SIMPLE FOOD HAZARD DETECTION - COMPATIBILITY MODE ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Configuration - Ultra simple\n",
    "CONFIG = {\n",
    "    'st1_task': True,  # Change to False for ST2\n",
    "    'use_bert': False,  # Set to True if you want to try BERT\n",
    "    'max_features': 10000,  # TF-IDF features\n",
    "    'test_bert_loading': False  # Test if BERT loading works\n",
    "}\n",
    "\n",
    "task_name = \"ST1\" if CONFIG['st1_task'] else \"ST2\"\n",
    "print(f\"Task: {task_name}\")\n",
    "print(f\"Method: {'BERT' if CONFIG['use_bert'] else 'TF-IDF + LogReg'}\")\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"\\n1. Loading data...\")\n",
    "train = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_train.csv?raw=true\")\n",
    "valid = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_valid.csv?raw=true\")\n",
    "test = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_test.csv?raw=true\")\n",
    "\n",
    "print(f\"‚úÖ Data loaded - Train: {len(train)}, Valid: {len(valid)}, Test: {len(test)}\")\n",
    "\n",
    "# Select columns based on task\n",
    "if CONFIG['st1_task']:\n",
    "    hazard_col = 'hazard-category'\n",
    "    product_col = 'product-category'\n",
    "else:\n",
    "    hazard_col = 'hazard'\n",
    "    product_col = 'product'\n",
    "\n",
    "print(f\"Target columns: {hazard_col}, {product_col}\")\n",
    "\n",
    "# 2. Data Analysis\n",
    "print(\"\\n2. Data analysis...\")\n",
    "hazard_counts = train[hazard_col].value_counts()\n",
    "product_counts = train[product_col].value_counts()\n",
    "\n",
    "print(f\"Hazard classes: {len(hazard_counts)}\")\n",
    "print(f\"Product classes: {len(product_counts)}\")\n",
    "print(f\"Most common hazard: {hazard_counts.index[0]} ({hazard_counts.iloc[0]} samples)\")\n",
    "print(f\"Most common product: {product_counts.index[0]} ({product_counts.iloc[0]} samples)\")\n",
    "\n",
    "# Check for imbalance\n",
    "imbalance_ratio_h = hazard_counts.iloc[0] / hazard_counts.iloc[-1]\n",
    "imbalance_ratio_p = product_counts.iloc[0] / product_counts.iloc[-1]\n",
    "print(f\"Hazard imbalance ratio: {imbalance_ratio_h:.1f}x\")\n",
    "print(f\"Product imbalance ratio: {imbalance_ratio_p:.1f}x\")\n",
    "\n",
    "# 3. Text preprocessing\n",
    "print(\"\\n3. Text preparation...\")\n",
    "\n",
    "def prepare_text(df):\n",
    "    \"\"\"Simple text preparation\"\"\"\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Combine title and text\n",
    "        text = str(row['title']) + \" \" + str(row.get('text', ''))\n",
    "        # Basic cleaning\n",
    "        text = text.lower().replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())  # Remove extra spaces\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "train_texts = prepare_text(train)\n",
    "valid_texts = prepare_text(valid)\n",
    "test_texts = prepare_text(test)\n",
    "\n",
    "print(f\"‚úÖ Texts prepared - Average length: {np.mean([len(t.split()) for t in train_texts[:100]]):.1f} words\")\n",
    "\n",
    "# 4. Test BERT loading if requested\n",
    "if CONFIG['test_bert_loading']:\n",
    "    print(\"\\n4. Testing BERT loading...\")\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')  # Lighter model\n",
    "        model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        print(\"‚úÖ BERT loading successful! You can set use_bert=True\")\n",
    "        CONFIG['use_bert'] = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BERT loading failed: {e}\")\n",
    "        print(\"Continuing with TF-IDF...\")\n",
    "        CONFIG['use_bert'] = False\n",
    "\n",
    "# 5. Model Training - TF-IDF Version (Always works)\n",
    "if not CONFIG['use_bert']:\n",
    "    print(\"\\n5. Training TF-IDF + LogReg model...\")\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=CONFIG['max_features'],\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    print(\"  Creating TF-IDF features...\")\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_valid = vectorizer.transform(valid_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    print(f\"  ‚úÖ TF-IDF shape: {X_train.shape}\")\n",
    "    \n",
    "    # Prepare labels\n",
    "    y_train_hazard = train[hazard_col].values\n",
    "    y_valid_hazard = valid[hazard_col].values\n",
    "    y_test_hazard = test[hazard_col].values\n",
    "    \n",
    "    y_train_product = train[product_col].values\n",
    "    y_valid_product = valid[product_col].values\n",
    "    y_test_product = test[product_col].values\n",
    "    \n",
    "    # Class weights for imbalanced data\n",
    "    hazard_classes = np.unique(y_train_hazard)\n",
    "    product_classes = np.unique(y_train_product)\n",
    "    \n",
    "    hazard_weights = compute_class_weight('balanced', classes=hazard_classes, y=y_train_hazard)\n",
    "    product_weights = compute_class_weight('balanced', classes=product_classes, y=y_train_product)\n",
    "    \n",
    "    hazard_weight_dict = dict(zip(hazard_classes, hazard_weights))\n",
    "    product_weight_dict = dict(zip(product_classes, product_weights))\n",
    "    \n",
    "    print(f\"  Class weights computed - Hazard: {len(hazard_weight_dict)}, Product: {len(product_weight_dict)}\")\n",
    "    \n",
    "    # Train models\n",
    "    print(\"  Training hazard classifier...\")\n",
    "    hazard_model = LogisticRegression(\n",
    "        class_weight=hazard_weight_dict,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    hazard_model.fit(X_train, y_train_hazard)\n",
    "    \n",
    "    print(\"  Training product classifier...\")\n",
    "    product_model = LogisticRegression(\n",
    "        class_weight=product_weight_dict,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    product_model.fit(X_train, y_train_product)\n",
    "    \n",
    "    print(\"  ‚úÖ Models trained\")\n",
    "    \n",
    "    # Predictions\n",
    "    print(\"\\n6. Making predictions...\")\n",
    "    hazard_pred_valid = hazard_model.predict(X_valid)\n",
    "    product_pred_valid = product_model.predict(X_valid)\n",
    "    \n",
    "    hazard_pred_test = hazard_model.predict(X_test)\n",
    "    product_pred_test = product_model.predict(X_test)\n",
    "\n",
    "# 6. Simple BERT Version (if compatible)\n",
    "elif CONFIG['use_bert']:\n",
    "    print(\"\\n5. Training Simple BERT...\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    # Simple dataset\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self, texts, hazard_labels, product_labels, tokenizer, max_length=128):\n",
    "            self.texts = texts\n",
    "            self.hazard_labels = hazard_labels\n",
    "            self.product_labels = product_labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Create label mappings\n",
    "            unique_hazards = sorted(list(set(hazard_labels)))\n",
    "            unique_products = sorted(list(set(product_labels)))\n",
    "            \n",
    "            self.hazard_to_id = {h: i for i, h in enumerate(unique_hazards)}\n",
    "            self.product_to_id = {p: i for i, p in enumerate(unique_products)}\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'hazard_label': torch.tensor(self.hazard_to_id[self.hazard_labels[idx]], dtype=torch.long),\n",
    "                'product_label': torch.tensor(self.product_to_id[self.product_labels[idx]], dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    # Simple BERT model\n",
    "    class SimpleBERT(nn.Module):\n",
    "        def __init__(self, model_name, num_hazards, num_products):\n",
    "            super().__init__()\n",
    "            self.bert = AutoModel.from_pretrained(model_name)\n",
    "            hidden_size = self.bert.config.hidden_size\n",
    "            self.hazard_head = nn.Linear(hidden_size, num_hazards)\n",
    "            self.product_head = nn.Linear(hidden_size, num_products)\n",
    "            \n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "            return self.hazard_head(pooled), self.product_head(pooled)\n",
    "    \n",
    "    # Initialize\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    train_dataset = SimpleDataset(train_texts, train[hazard_col].values, train[product_col].values, tokenizer)\n",
    "    valid_dataset = SimpleDataset(valid_texts, valid[hazard_col].values, valid[product_col].values, tokenizer)\n",
    "    test_dataset = SimpleDataset(test_texts, test[hazard_col].values, test[product_col].values, tokenizer)\n",
    "    \n",
    "    model = SimpleBERT('distilbert-base-uncased', len(train_dataset.hazard_to_id), len(train_dataset.product_to_id))\n",
    "    \n",
    "    # Simple training (just 1 epoch for demo)\n",
    "    device = torch.device('cpu')  # Force CPU to avoid device issues\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"  Training (1 epoch for demo)...\")\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if batch_idx > 50:  # Just first 50 batches for demo\n",
    "            break\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        hazard_labels = batch['hazard_label'].to(device)\n",
    "        product_labels = batch['product_label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hazard_logits, product_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(hazard_logits, hazard_labels) + criterion(product_logits, product_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"    Batch {batch_idx}/50, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"  ‚úÖ BERT training completed (demo)\")\n",
    "    \n",
    "    # Simple predictions for BERT would go here...\n",
    "    # For now, fall back to TF-IDF results\n",
    "    print(\"  Note: Using TF-IDF results for evaluation\")\n",
    "\n",
    "# 7. Evaluation Function\n",
    "def compute_food_hazard_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    \"\"\"Official SemEval scoring function\"\"\"\n",
    "    f1_hazards = f1_score(hazards_true, hazards_pred, average='macro')\n",
    "    \n",
    "    correct_hazard_mask = hazards_pred == hazards_true\n",
    "    if sum(correct_hazard_mask) > 0:\n",
    "        f1_products = f1_score(\n",
    "            products_true[correct_hazard_mask],\n",
    "            products_pred[correct_hazard_mask],\n",
    "            average='macro'\n",
    "        )\n",
    "    else:\n",
    "        f1_products = 0.0\n",
    "    \n",
    "    return {\n",
    "        'f1_hazards': f1_hazards,\n",
    "        'f1_products': f1_products,\n",
    "        'final_score': (f1_hazards + f1_products) / 2\n",
    "    }\n",
    "\n",
    "# 8. Results\n",
    "print(\"\\n7. Evaluation...\")\n",
    "\n",
    "# Validation results\n",
    "valid_scores = compute_food_hazard_score(\n",
    "    y_valid_hazard, y_valid_product,\n",
    "    hazard_pred_valid, product_pred_valid\n",
    ")\n",
    "\n",
    "# Test results  \n",
    "test_scores = compute_food_hazard_score(\n",
    "    y_test_hazard, y_test_product,\n",
    "    hazard_pred_test, product_pred_test\n",
    ")\n",
    "\n",
    "print(\"\\n=== VALIDATION RESULTS ===\")\n",
    "print(f\"Hazard F1: {valid_scores['f1_hazards']:.4f}\")\n",
    "print(f\"Product F1: {valid_scores['f1_products']:.4f}\")\n",
    "print(f\"Final Score: {valid_scores['final_score']:.4f}\")\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "print(f\"Hazard F1: {test_scores['f1_hazards']:.4f}\")\n",
    "print(f\"Product F1: {test_scores['f1_products']:.4f}\")\n",
    "print(f\"Final Score: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "# 9. Comparison with baselines\n",
    "print(\"\\n8. Baseline comparison...\")\n",
    "\n",
    "# Majority classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_hazard = DummyClassifier(strategy='most_frequent')\n",
    "dummy_product = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "dummy_hazard.fit(X_train, y_train_hazard)\n",
    "dummy_product.fit(X_train, y_train_product)\n",
    "\n",
    "dummy_hazard_pred = dummy_hazard.predict(X_test)\n",
    "dummy_product_pred = dummy_product.predict(X_test)\n",
    "\n",
    "dummy_scores = compute_food_hazard_score(\n",
    "    y_test_hazard, y_test_product,\n",
    "    dummy_hazard_pred, dummy_product_pred\n",
    ")\n",
    "\n",
    "print(f\"Majority Baseline: {dummy_scores['final_score']:.4f}\")\n",
    "print(f\"Our Model: {test_scores['final_score']:.4f}\")\n",
    "print(f\"Improvement: +{test_scores['final_score'] - dummy_scores['final_score']:.4f}\")\n",
    "\n",
    "# 10. Competition comparison\n",
    "print(f\"\\n=== COMPETITION COMPARISON ({task_name}) ===\")\n",
    "if CONFIG['st1_task']:\n",
    "    competition_best = 0.8223\n",
    "    competition_bert = 0.667\n",
    "    print(f\"Competition Best (Anastasia): {competition_best:.4f}\")\n",
    "    print(f\"Competition BERT Baseline: {competition_bert:.4f}\")\n",
    "else:\n",
    "    competition_best = 0.5473\n",
    "    competition_bert = 0.498\n",
    "    print(f\"Competition Best (SRCB): {competition_best:.4f}\")\n",
    "    print(f\"Competition BERT Baseline: {competition_bert:.4f}\")\n",
    "\n",
    "print(f\"Your Result: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "if CONFIG['st1_task']:\n",
    "    if test_scores['final_score'] > competition_bert:\n",
    "        print(f\"üéâ You beat the BERT baseline by {test_scores['final_score'] - competition_bert:.4f}!\")\n",
    "    gap_to_best = competition_best - test_scores['final_score']\n",
    "    print(f\"Gap to best: {gap_to_best:.4f}\")\n",
    "else:\n",
    "    if test_scores['final_score'] > competition_bert:\n",
    "        print(f\"üéâ You beat the BERT baseline by {test_scores['final_score'] - competition_bert:.4f}!\")\n",
    "    gap_to_best = competition_best - test_scores['final_score']\n",
    "    print(f\"Gap to best: {gap_to_best:.4f}\")\n",
    "\n",
    "# 11. Save results\n",
    "results_summary = {\n",
    "    'task': task_name,\n",
    "    'method': 'TF-IDF + LogReg with Class Weights',\n",
    "    'config': CONFIG,\n",
    "    'data_stats': {\n",
    "        'train_size': len(train),\n",
    "        'valid_size': len(valid),\n",
    "        'test_size': len(test),\n",
    "        'hazard_classes': len(hazard_counts),\n",
    "        'product_classes': len(product_counts),\n",
    "        'hazard_imbalance': float(imbalance_ratio_h),\n",
    "        'product_imbalance': float(imbalance_ratio_p)\n",
    "    },\n",
    "    'results': {\n",
    "        'validation': valid_scores,\n",
    "        'test': test_scores,\n",
    "        'baseline': dummy_scores,\n",
    "        'improvement': float(test_scores['final_score'] - dummy_scores['final_score'])\n",
    "    },\n",
    "    'competition_comparison': {\n",
    "        'competition_best': float(competition_best),\n",
    "        'competition_bert': float(competition_bert),\n",
    "        'our_result': float(test_scores['final_score']),\n",
    "        'gap_to_best': float(competition_best - test_scores['final_score'])\n",
    "    }\n",
    "}\n",
    "\n",
    "filename = f'simple_results_{task_name.lower()}.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to {filename}\")\n",
    "\n",
    "print(\"\\n=== EXPERIMENT COMPLETED ===\")\n",
    "print(f\"Method: TF-IDF + Logistic Regression\")\n",
    "print(f\"Final {task_name} Score: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "# Quick suggestions for improvement\n",
    "print(f\"\\nüí° NEXT STEPS FOR BETTER RESULTS:\")\n",
    "print(f\"1. Try different TF-IDF parameters (max_features, ngram_range)\")\n",
    "print(f\"2. Add more feature engineering (text length, country, date)\")\n",
    "print(f\"3. Try ensemble methods (combine multiple models)\")\n",
    "print(f\"4. If BERT works, try: 'test_bert_loading': True\")\n",
    "print(f\"5. For ST2 task, set: 'st1_task': False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a706ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE FOOD HAZARD DETECTION - COMPATIBILITY MODE ===\n",
      "PyTorch version: 2.2.2\n",
      "Task: ST2\n",
      "Method: TF-IDF + LogReg\n",
      "\n",
      "1. Loading data...\n",
      "‚úÖ Data loaded - Train: 5082, Valid: 565, Test: 997\n",
      "Target columns: hazard, product\n",
      "\n",
      "2. Data analysis...\n",
      "Hazard classes: 128\n",
      "Product classes: 1022\n",
      "Most common hazard: listeria monocytogenes (665 samples)\n",
      "Most common product: ice cream (185 samples)\n",
      "Hazard imbalance ratio: 221.7x\n",
      "Product imbalance ratio: 185.0x\n",
      "\n",
      "3. Text preparation...\n",
      "‚úÖ Texts prepared - Average length: 169.4 words\n",
      "\n",
      "5. Training TF-IDF + LogReg model...\n",
      "  Creating TF-IDF features...\n",
      "  ‚úÖ TF-IDF shape: (5082, 10000)\n",
      "  Class weights computed - Hazard: 128, Product: 1022\n",
      "  Training hazard classifier...\n",
      "  Training product classifier...\n",
      "  ‚úÖ Models trained\n",
      "\n",
      "6. Making predictions...\n",
      "\n",
      "7. Evaluation...\n",
      "\n",
      "=== VALIDATION RESULTS ===\n",
      "Hazard F1: 0.3874\n",
      "Product F1: 0.0797\n",
      "Final Score: 0.2336\n",
      "\n",
      "=== TEST RESULTS ===\n",
      "Hazard F1: 0.4241\n",
      "Product F1: 0.0850\n",
      "Final Score: 0.2546\n",
      "\n",
      "8. Baseline comparison...\n",
      "Majority Baseline: 0.0014\n",
      "Our Model: 0.2546\n",
      "Improvement: +0.2531\n",
      "\n",
      "=== COMPETITION COMPARISON (ST2) ===\n",
      "Competition Best (SRCB): 0.5473\n",
      "Competition BERT Baseline: 0.4980\n",
      "Your Result: 0.2546\n",
      "Gap to best: 0.2927\n",
      "\n",
      "‚úÖ Results saved to simple_results_st2.json\n",
      "\n",
      "=== EXPERIMENT COMPLETED ===\n",
      "Method: TF-IDF + Logistic Regression\n",
      "Final ST2 Score: 0.2546\n",
      "\n",
      "üí° NEXT STEPS FOR BETTER RESULTS:\n",
      "1. Try different TF-IDF parameters (max_features, ngram_range)\n",
      "2. Add more feature engineering (text length, country, date)\n",
      "3. Try ensemble methods (combine multiple models)\n",
      "4. If BERT works, try: 'test_bert_loading': True\n",
      "5. For ST2 task, set: 'st1_task': False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== SIMPLE FOOD HAZARD DETECTION - COMPATIBILITY MODE ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Configuration - Ultra simple\n",
    "CONFIG = {\n",
    "    'st1_task': False,  # Change to False for ST2\n",
    "    'use_bert': False,  # Set to True if you want to try BERT\n",
    "    'max_features': 10000,  # TF-IDF features\n",
    "    'test_bert_loading': False  # Test if BERT loading works\n",
    "}\n",
    "\n",
    "task_name = \"ST1\" if CONFIG['st1_task'] else \"ST2\"\n",
    "print(f\"Task: {task_name}\")\n",
    "print(f\"Method: {'BERT' if CONFIG['use_bert'] else 'TF-IDF + LogReg'}\")\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"\\n1. Loading data...\")\n",
    "train = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_train.csv?raw=true\")\n",
    "valid = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_valid.csv?raw=true\")\n",
    "test = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_test.csv?raw=true\")\n",
    "\n",
    "print(f\"‚úÖ Data loaded - Train: {len(train)}, Valid: {len(valid)}, Test: {len(test)}\")\n",
    "\n",
    "# Select columns based on task\n",
    "if CONFIG['st1_task']:\n",
    "    hazard_col = 'hazard-category'\n",
    "    product_col = 'product-category'\n",
    "else:\n",
    "    hazard_col = 'hazard'\n",
    "    product_col = 'product'\n",
    "\n",
    "print(f\"Target columns: {hazard_col}, {product_col}\")\n",
    "\n",
    "# 2. Data Analysis\n",
    "print(\"\\n2. Data analysis...\")\n",
    "hazard_counts = train[hazard_col].value_counts()\n",
    "product_counts = train[product_col].value_counts()\n",
    "\n",
    "print(f\"Hazard classes: {len(hazard_counts)}\")\n",
    "print(f\"Product classes: {len(product_counts)}\")\n",
    "print(f\"Most common hazard: {hazard_counts.index[0]} ({hazard_counts.iloc[0]} samples)\")\n",
    "print(f\"Most common product: {product_counts.index[0]} ({product_counts.iloc[0]} samples)\")\n",
    "\n",
    "# Check for imbalance\n",
    "imbalance_ratio_h = hazard_counts.iloc[0] / hazard_counts.iloc[-1]\n",
    "imbalance_ratio_p = product_counts.iloc[0] / product_counts.iloc[-1]\n",
    "print(f\"Hazard imbalance ratio: {imbalance_ratio_h:.1f}x\")\n",
    "print(f\"Product imbalance ratio: {imbalance_ratio_p:.1f}x\")\n",
    "\n",
    "# 3. Text preprocessing\n",
    "print(\"\\n3. Text preparation...\")\n",
    "\n",
    "def prepare_text(df):\n",
    "    \"\"\"Simple text preparation\"\"\"\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Combine title and text\n",
    "        text = str(row['title']) + \" \" + str(row.get('text', ''))\n",
    "        # Basic cleaning\n",
    "        text = text.lower().replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = ' '.join(text.split())  # Remove extra spaces\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "train_texts = prepare_text(train)\n",
    "valid_texts = prepare_text(valid)\n",
    "test_texts = prepare_text(test)\n",
    "\n",
    "print(f\"‚úÖ Texts prepared - Average length: {np.mean([len(t.split()) for t in train_texts[:100]]):.1f} words\")\n",
    "\n",
    "# 4. Test BERT loading if requested\n",
    "if CONFIG['test_bert_loading']:\n",
    "    print(\"\\n4. Testing BERT loading...\")\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')  # Lighter model\n",
    "        model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        print(\"‚úÖ BERT loading successful! You can set use_bert=True\")\n",
    "        CONFIG['use_bert'] = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå BERT loading failed: {e}\")\n",
    "        print(\"Continuing with TF-IDF...\")\n",
    "        CONFIG['use_bert'] = False\n",
    "\n",
    "# 5. Model Training - TF-IDF Version (Always works)\n",
    "if not CONFIG['use_bert']:\n",
    "    print(\"\\n5. Training TF-IDF + LogReg model...\")\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=CONFIG['max_features'],\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    print(\"  Creating TF-IDF features...\")\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_valid = vectorizer.transform(valid_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "    \n",
    "    print(f\"  ‚úÖ TF-IDF shape: {X_train.shape}\")\n",
    "    \n",
    "    # Prepare labels\n",
    "    y_train_hazard = train[hazard_col].values\n",
    "    y_valid_hazard = valid[hazard_col].values\n",
    "    y_test_hazard = test[hazard_col].values\n",
    "    \n",
    "    y_train_product = train[product_col].values\n",
    "    y_valid_product = valid[product_col].values\n",
    "    y_test_product = test[product_col].values\n",
    "    \n",
    "    # Class weights for imbalanced data\n",
    "    hazard_classes = np.unique(y_train_hazard)\n",
    "    product_classes = np.unique(y_train_product)\n",
    "    \n",
    "    hazard_weights = compute_class_weight('balanced', classes=hazard_classes, y=y_train_hazard)\n",
    "    product_weights = compute_class_weight('balanced', classes=product_classes, y=y_train_product)\n",
    "    \n",
    "    hazard_weight_dict = dict(zip(hazard_classes, hazard_weights))\n",
    "    product_weight_dict = dict(zip(product_classes, product_weights))\n",
    "    \n",
    "    print(f\"  Class weights computed - Hazard: {len(hazard_weight_dict)}, Product: {len(product_weight_dict)}\")\n",
    "    \n",
    "    # Train models\n",
    "    print(\"  Training hazard classifier...\")\n",
    "    hazard_model = LogisticRegression(\n",
    "        class_weight=hazard_weight_dict,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    hazard_model.fit(X_train, y_train_hazard)\n",
    "    \n",
    "    print(\"  Training product classifier...\")\n",
    "    product_model = LogisticRegression(\n",
    "        class_weight=product_weight_dict,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    product_model.fit(X_train, y_train_product)\n",
    "    \n",
    "    print(\"  ‚úÖ Models trained\")\n",
    "    \n",
    "    # Predictions\n",
    "    print(\"\\n6. Making predictions...\")\n",
    "    hazard_pred_valid = hazard_model.predict(X_valid)\n",
    "    product_pred_valid = product_model.predict(X_valid)\n",
    "    \n",
    "    hazard_pred_test = hazard_model.predict(X_test)\n",
    "    product_pred_test = product_model.predict(X_test)\n",
    "\n",
    "# 6. Simple BERT Version (if compatible)\n",
    "elif CONFIG['use_bert']:\n",
    "    print(\"\\n5. Training Simple BERT...\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    # Simple dataset\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self, texts, hazard_labels, product_labels, tokenizer, max_length=128):\n",
    "            self.texts = texts\n",
    "            self.hazard_labels = hazard_labels\n",
    "            self.product_labels = product_labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Create label mappings\n",
    "            unique_hazards = sorted(list(set(hazard_labels)))\n",
    "            unique_products = sorted(list(set(product_labels)))\n",
    "            \n",
    "            self.hazard_to_id = {h: i for i, h in enumerate(unique_hazards)}\n",
    "            self.product_to_id = {p: i for i, p in enumerate(unique_products)}\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            \n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'hazard_label': torch.tensor(self.hazard_to_id[self.hazard_labels[idx]], dtype=torch.long),\n",
    "                'product_label': torch.tensor(self.product_to_id[self.product_labels[idx]], dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    # Simple BERT model\n",
    "    class SimpleBERT(nn.Module):\n",
    "        def __init__(self, model_name, num_hazards, num_products):\n",
    "            super().__init__()\n",
    "            self.bert = AutoModel.from_pretrained(model_name)\n",
    "            hidden_size = self.bert.config.hidden_size\n",
    "            self.hazard_head = nn.Linear(hidden_size, num_hazards)\n",
    "            self.product_head = nn.Linear(hidden_size, num_products)\n",
    "            \n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "            return self.hazard_head(pooled), self.product_head(pooled)\n",
    "    \n",
    "    # Initialize\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    train_dataset = SimpleDataset(train_texts, train[hazard_col].values, train[product_col].values, tokenizer)\n",
    "    valid_dataset = SimpleDataset(valid_texts, valid[hazard_col].values, valid[product_col].values, tokenizer)\n",
    "    test_dataset = SimpleDataset(test_texts, test[hazard_col].values, test[product_col].values, tokenizer)\n",
    "    \n",
    "    model = SimpleBERT('distilbert-base-uncased', len(train_dataset.hazard_to_id), len(train_dataset.product_to_id))\n",
    "    \n",
    "    # Simple training (just 1 epoch for demo)\n",
    "    device = torch.device('cpu')  # Force CPU to avoid device issues\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"  Training (1 epoch for demo)...\")\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        if batch_idx > 50:  # Just first 50 batches for demo\n",
    "            break\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        hazard_labels = batch['hazard_label'].to(device)\n",
    "        product_labels = batch['product_label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hazard_logits, product_logits = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(hazard_logits, hazard_labels) + criterion(product_logits, product_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"    Batch {batch_idx}/50, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"  ‚úÖ BERT training completed (demo)\")\n",
    "    \n",
    "    # Simple predictions for BERT would go here...\n",
    "    # For now, fall back to TF-IDF results\n",
    "    print(\"  Note: Using TF-IDF results for evaluation\")\n",
    "\n",
    "# 7. Evaluation Function\n",
    "def compute_food_hazard_score(hazards_true, products_true, hazards_pred, products_pred):\n",
    "    \"\"\"Official SemEval scoring function\"\"\"\n",
    "    f1_hazards = f1_score(hazards_true, hazards_pred, average='macro')\n",
    "    \n",
    "    correct_hazard_mask = hazards_pred == hazards_true\n",
    "    if sum(correct_hazard_mask) > 0:\n",
    "        f1_products = f1_score(\n",
    "            products_true[correct_hazard_mask],\n",
    "            products_pred[correct_hazard_mask],\n",
    "            average='macro'\n",
    "        )\n",
    "    else:\n",
    "        f1_products = 0.0\n",
    "    \n",
    "    return {\n",
    "        'f1_hazards': f1_hazards,\n",
    "        'f1_products': f1_products,\n",
    "        'final_score': (f1_hazards + f1_products) / 2\n",
    "    }\n",
    "\n",
    "# 8. Results\n",
    "print(\"\\n7. Evaluation...\")\n",
    "\n",
    "# Validation results\n",
    "valid_scores = compute_food_hazard_score(\n",
    "    y_valid_hazard, y_valid_product,\n",
    "    hazard_pred_valid, product_pred_valid\n",
    ")\n",
    "\n",
    "# Test results  \n",
    "test_scores = compute_food_hazard_score(\n",
    "    y_test_hazard, y_test_product,\n",
    "    hazard_pred_test, product_pred_test\n",
    ")\n",
    "\n",
    "print(\"\\n=== VALIDATION RESULTS ===\")\n",
    "print(f\"Hazard F1: {valid_scores['f1_hazards']:.4f}\")\n",
    "print(f\"Product F1: {valid_scores['f1_products']:.4f}\")\n",
    "print(f\"Final Score: {valid_scores['final_score']:.4f}\")\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "print(f\"Hazard F1: {test_scores['f1_hazards']:.4f}\")\n",
    "print(f\"Product F1: {test_scores['f1_products']:.4f}\")\n",
    "print(f\"Final Score: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "# 9. Comparison with baselines\n",
    "print(\"\\n8. Baseline comparison...\")\n",
    "\n",
    "# Majority classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_hazard = DummyClassifier(strategy='most_frequent')\n",
    "dummy_product = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "dummy_hazard.fit(X_train, y_train_hazard)\n",
    "dummy_product.fit(X_train, y_train_product)\n",
    "\n",
    "dummy_hazard_pred = dummy_hazard.predict(X_test)\n",
    "dummy_product_pred = dummy_product.predict(X_test)\n",
    "\n",
    "dummy_scores = compute_food_hazard_score(\n",
    "    y_test_hazard, y_test_product,\n",
    "    dummy_hazard_pred, dummy_product_pred\n",
    ")\n",
    "\n",
    "print(f\"Majority Baseline: {dummy_scores['final_score']:.4f}\")\n",
    "print(f\"Our Model: {test_scores['final_score']:.4f}\")\n",
    "print(f\"Improvement: +{test_scores['final_score'] - dummy_scores['final_score']:.4f}\")\n",
    "\n",
    "# 10. Competition comparison\n",
    "print(f\"\\n=== COMPETITION COMPARISON ({task_name}) ===\")\n",
    "if CONFIG['st1_task']:\n",
    "    competition_best = 0.8223\n",
    "    competition_bert = 0.667\n",
    "    print(f\"Competition Best (Anastasia): {competition_best:.4f}\")\n",
    "    print(f\"Competition BERT Baseline: {competition_bert:.4f}\")\n",
    "else:\n",
    "    competition_best = 0.5473\n",
    "    competition_bert = 0.498\n",
    "    print(f\"Competition Best (SRCB): {competition_best:.4f}\")\n",
    "    print(f\"Competition BERT Baseline: {competition_bert:.4f}\")\n",
    "\n",
    "print(f\"Your Result: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "if CONFIG['st1_task']:\n",
    "    if test_scores['final_score'] > competition_bert:\n",
    "        print(f\"üéâ You beat the BERT baseline by {test_scores['final_score'] - competition_bert:.4f}!\")\n",
    "    gap_to_best = competition_best - test_scores['final_score']\n",
    "    print(f\"Gap to best: {gap_to_best:.4f}\")\n",
    "else:\n",
    "    if test_scores['final_score'] > competition_bert:\n",
    "        print(f\"üéâ You beat the BERT baseline by {test_scores['final_score'] - competition_bert:.4f}!\")\n",
    "    gap_to_best = competition_best - test_scores['final_score']\n",
    "    print(f\"Gap to best: {gap_to_best:.4f}\")\n",
    "\n",
    "# 11. Save results\n",
    "results_summary = {\n",
    "    'task': task_name,\n",
    "    'method': 'TF-IDF + LogReg with Class Weights',\n",
    "    'config': CONFIG,\n",
    "    'data_stats': {\n",
    "        'train_size': len(train),\n",
    "        'valid_size': len(valid),\n",
    "        'test_size': len(test),\n",
    "        'hazard_classes': len(hazard_counts),\n",
    "        'product_classes': len(product_counts),\n",
    "        'hazard_imbalance': float(imbalance_ratio_h),\n",
    "        'product_imbalance': float(imbalance_ratio_p)\n",
    "    },\n",
    "    'results': {\n",
    "        'validation': valid_scores,\n",
    "        'test': test_scores,\n",
    "        'baseline': dummy_scores,\n",
    "        'improvement': float(test_scores['final_score'] - dummy_scores['final_score'])\n",
    "    },\n",
    "    'competition_comparison': {\n",
    "        'competition_best': float(competition_best),\n",
    "        'competition_bert': float(competition_bert),\n",
    "        'our_result': float(test_scores['final_score']),\n",
    "        'gap_to_best': float(competition_best - test_scores['final_score'])\n",
    "    }\n",
    "}\n",
    "\n",
    "filename = f'simple_results_{task_name.lower()}.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to {filename}\")\n",
    "\n",
    "print(\"\\n=== EXPERIMENT COMPLETED ===\")\n",
    "print(f\"Method: TF-IDF + Logistic Regression\")\n",
    "print(f\"Final {task_name} Score: {test_scores['final_score']:.4f}\")\n",
    "\n",
    "# Quick suggestions for improvement\n",
    "print(f\"\\nüí° NEXT STEPS FOR BETTER RESULTS:\")\n",
    "print(f\"1. Try different TF-IDF parameters (max_features, ngram_range)\")\n",
    "print(f\"2. Add more feature engineering (text length, country, date)\")\n",
    "print(f\"3. Try ensemble methods (combine multiple models)\")\n",
    "print(f\"4. If BERT works, try: 'test_bert_loading': True\")\n",
    "print(f\"5. For ST2 task, set: 'st1_task': False\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
