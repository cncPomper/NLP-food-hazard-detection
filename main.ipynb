{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc5130e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/michal/miniconda/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/michal/miniconda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in /Users/michal/miniconda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/michal/miniconda/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: hf_xet in /Users/michal/miniconda/lib/python3.12/site-packages (1.0.3)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/michal/miniconda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: datasets in /Users/michal/miniconda/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michal/miniconda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/michal/miniconda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in /Users/michal/miniconda/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/michal/miniconda/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: sympy in /Users/michal/miniconda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/michal/miniconda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/michal/miniconda/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/michal/miniconda/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/michal/miniconda/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/michal/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michal/miniconda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michal/miniconda/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas transformers scikit-learn hf_xet 'accelerate>=0.26.0' datasets\n",
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217033c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_test.csv?raw=true\")\n",
    "train = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_train.csv?raw=true\")\n",
    "valid = pd.read_csv(\"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_valid.csv?raw=true\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e87fa2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Analyzing dataset...\n",
      "Number of hazard categories: 10\n",
      "Number of product categories: 22\n",
      "Tokenizing dataset...\n",
      "Initializing model...\n",
      "Training model...\n",
      "Using device: cpu\n",
      "Epoch 1/3, Loss: 2.7119\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 240\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# Evaluate after each epoch\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 131\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, device)\u001b[0m\n\u001b[1;32m    128\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m hazard_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhazard_logits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m product_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    134\u001b[0m all_hazard_preds\u001b[38;5;241m.\u001b[39mextend(hazard_preds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# 1. Load and Process Dataset\n",
    "print(\"Loading dataset...\")\n",
    "data_files = {\n",
    "    \"train\": \"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_train.csv?raw=true\", \n",
    "    \"test\": \"https://github.com/food-hazard-detection-semeval-2025/food-hazard-detection-semeval-2025.github.io/blob/main/data/incidents_test.csv?raw=true\"\n",
    "}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# 2. Analyze and Preprocess Dataset\n",
    "print(\"Analyzing dataset...\")\n",
    "\n",
    "# Examine unique values in key columns\n",
    "hazard_categories = dataset[\"train\"].unique(\"hazard-category\")\n",
    "product_categories = dataset[\"train\"].unique(\"product-category\")\n",
    "\n",
    "print(f\"Number of hazard categories: {len(hazard_categories)}\")\n",
    "print(f\"Number of product categories: {len(product_categories)}\")\n",
    "\n",
    "# Create label mappings\n",
    "hazard_label_mapping = {cat: idx for idx, cat in enumerate(hazard_categories)}\n",
    "product_label_mapping = {cat: idx for idx, cat in enumerate(product_categories)}\n",
    "\n",
    "# Map string labels to integers\n",
    "def map_labels(example):\n",
    "    hazard_label = hazard_label_mapping.get(example[\"hazard-category\"], -1)\n",
    "    product_label = product_label_mapping.get(example[\"product-category\"], -1)\n",
    "    return {\"hazard_label\": hazard_label, \"product_label\": product_label}\n",
    "\n",
    "dataset = dataset.map(map_labels)\n",
    "\n",
    "# 3. Tokenize Dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Using only the title field for tokenization as specified\n",
    "    return tokenizer(\n",
    "        examples[\"title\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128  # As specified in requirements\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 4. Custom BERT Model with Two Classification Heads\n",
    "class BERTWithDualHeads(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_hazards, num_products):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Two classification heads\n",
    "        self.hazard_classifier = nn.Linear(self.bert.config.hidden_size, num_hazards)\n",
    "        self.product_classifier = nn.Linear(self.bert.config.hidden_size, num_products)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get the [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Pass through the classification heads\n",
    "        hazard_logits = self.hazard_classifier(pooled_output)\n",
    "        product_logits = self.product_classifier(pooled_output)\n",
    "        \n",
    "        return {\n",
    "            \"hazard_logits\": hazard_logits,\n",
    "            \"product_logits\": product_logits\n",
    "        }\n",
    "\n",
    "# 5. Custom Dataset for Dual Classification\n",
    "class DualClassificationDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized_dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"]),\n",
    "            \"hazard_label\": torch.tensor(item[\"hazard_label\"]),\n",
    "            \"product_label\": torch.tensor(item[\"product_label\"])\n",
    "        }\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = DualClassificationDataset(tokenized_datasets[\"train\"])\n",
    "test_dataset = DualClassificationDataset(tokenized_datasets[\"test\"])\n",
    "\n",
    "# 7. Evaluation Function\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_hazard_preds = []\n",
    "    all_product_preds = []\n",
    "    all_hazard_true = []\n",
    "    all_product_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Get predictions\n",
    "            hazard_preds = torch.argmax(outputs[\"hazard_logits\"], dim=1).cpu().numpy()\n",
    "            product_preds = torch.argmax(outputs[\"product_logits\"], dim=1).cpu().numpy()\n",
    "            \n",
    "            all_hazard_preds.extend(hazard_preds)\n",
    "            all_product_preds.extend(product_preds)\n",
    "            all_hazard_true.extend(batch[\"hazard_label\"].numpy())\n",
    "            all_product_true.extend(batch[\"product_label\"].numpy())\n",
    "    \n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    all_hazard_preds = np.array(all_hazard_preds)\n",
    "    all_product_preds = np.array(all_product_preds)\n",
    "    all_hazard_true = np.array(all_hazard_true)\n",
    "    all_product_true = np.array(all_product_true)\n",
    "    \n",
    "    # Calculate macro-F1 scores\n",
    "    f1_hazard = f1_score(all_hazard_true, all_hazard_preds, average='macro')\n",
    "    \n",
    "    # Calculate product F1 score only for instances where hazard prediction is correct\n",
    "    correct_hazard_mask = all_hazard_preds == all_hazard_true\n",
    "    \n",
    "    if sum(correct_hazard_mask) > 0:\n",
    "        f1_product = f1_score(\n",
    "            all_product_true[correct_hazard_mask], \n",
    "            all_product_preds[correct_hazard_mask], \n",
    "            average='macro'\n",
    "        )\n",
    "    else:\n",
    "        f1_product = 0.0\n",
    "    \n",
    "    # Final score as per the requirement\n",
    "    final_score = (f1_hazard + f1_product) / 2\n",
    "    \n",
    "    print(f\"Hazard Macro-F1: {f1_hazard:.4f}\")\n",
    "    print(f\"Product Macro-F1 (for correct hazards): {f1_product:.4f}\")\n",
    "    print(f\"Final Score: {final_score:.4f}\")\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparams = {\n",
    "    \"learning_rate\": 2e-5,  # As specified in requirements\n",
    "    \"batch_size\": 16,       # As specified in requirements\n",
    "    \"epochs\": 3            # As specified in requirements\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "model = BERTWithDualHeads(\n",
    "    bert_model_name=\"bert-base-uncased\", \n",
    "    num_hazards=len(hazard_categories),  # 10 hazard categories\n",
    "    num_products=len(product_categories)  # 22 product categories\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparams[\"learning_rate\"])\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=hyperparams[\"batch_size\"], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=hyperparams[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(hyperparams[\"epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        hazard_labels = batch[\"hazard_label\"].to(device)\n",
    "        product_labels = batch[\"product_label\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Calculate loss\n",
    "        hazard_loss = loss_fn(outputs[\"hazard_logits\"], hazard_labels)\n",
    "        product_loss = loss_fn(outputs[\"product_logits\"], product_labels)\n",
    "        loss = hazard_loss + product_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{hyperparams['epochs']}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate after each epoch\n",
    "    evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model...\")\n",
    "torch.save(model.state_dict(), \"bert_food_hazard_model.pt\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
